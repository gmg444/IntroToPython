{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring IRS 990 XMLs with Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first set of notebooks, we went over the basic parts of Python that are commonly used for data processing tasks.  In this set, we'll go over some useful libraries and addons, using some concrete examples. \n",
    "\n",
    "We'll start by seeing what we can do with the publicly-available IRS 990s, downloading and parsing the data, enriching it with ancillary data, exploring and analyzing it further, and building a web service to share the results.\n",
    "\n",
    "General information about the IRS 990 forms is available here.\n",
    "\n",
    "https://aws.amazon.com/public-datasets/irs-990/\n",
    "\n",
    "The data is made available via an index file that lists the returns for each year.  \n",
    "\n",
    "https://s3.amazonaws.com/irs-form-990/index_2016.json\n",
    "\n",
    "You might want to install a chrome extension to format the json in a viewable form:\n",
    "\n",
    "https://chrome.google.com/webstore/detail/xv-%E2%80%94-xml-viewer/eeocglpgjdpaefaedpblffpeebgmgddk?utm_source=chrome-app-launcher-info-dialog\n",
    "\n",
    "The returns themselves are in XML format.  Here's an example:\n",
    "\n",
    "https://s3.amazonaws.com/irs-form-990/201523229349300307_public.xml\n",
    "\n",
    "Similarly, a browser plugin that formats XML can be helpful:\n",
    "\n",
    "https://chrome.google.com/webstore/detail/xv-%E2%80%94-xml-viewer/eeocglpgjdpaefaedpblffpeebgmgddk?utm_source=chrome-app-launcher-info-dialog\n",
    "\n",
    "The return format changes from year to year and between return types.  For simplicity's sake, we can just focus on 2015 990s as listed in the 2016 index file.  The general outline of steps we need to take is:\n",
    "\n",
    "1. Figure out which urls to access. The index file for each year can contain returns from multiple years and return types, so we need to get the urls only for 2016 and where the return type is 990. \n",
    "2. Open up the XML and pull out the fields we're interested in. To do this, we have to figure out where in the XML structure these fields are:\n",
    "3. Save these locally.\n",
    "4. Extract some usable information from the data, or append some ancillary data.\n",
    "5. Store and retrieve the data.\n",
    "\n",
    "Step 4 will take some iteration to see what's possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from a URL\n",
    "\n",
    "The first step is to figure out what URLs to download.  We can use the IRS index file to get these urls.  The following snippet has some steps that need to be completed. The result should be a file containing the URLs we're interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Get the index file.\n",
    "response = requests.get(\"https://s3.amazonaws.com/irs-form-990/index_2016.json\")\n",
    "# Load it into a Python dictionary.\n",
    "data = json.loads(response.text)\n",
    "urls = []\n",
    "# Go through each item and pull out the rows:\n",
    "for return_metadata in data[\"Filings2016\"]:\n",
    "    # Code that needs to be completed - only look at records where TaxPeriod starts with \"2015\" and FormType = \"990\"\n",
    "    if return_metadata[\"TaxPeriod\"].startswith(\"2015\") and return_metadata[\"FormType\"] == \"990\":\n",
    "        urls.append(return_metadata[\"URL\"] + \"\\n\")\n",
    "\n",
    "# Write out the urls to a file\n",
    "with open(\"urls.txt\", \"w\") as f:\n",
    "    f.writelines(urls)\n",
    "\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the fielded data from the XML\n",
    "Now that we know what URLs to check, we can go through these and get data from the XML files.  The _**xmltodict**_ python module can parse the XML into a dictionary for us.  Some fields we might find interesting are:\n",
    "\n",
    "* **EIN**:     ```Return->ReturnHeader->Filer->EIN```\n",
    "* **Name**:    ```Return->ReturnHeader->Filer->BusinessName->BusinessNameLine1Txt```\n",
    "* **Address**: ```Return->ReturnHeader->Filer->USAddress->AddressLine1Txt```\n",
    "* **City**:    ```Return->ReturnHeader->Filer->USAddress->CityNm```\n",
    "* **State**:   ```Return->ReturnHeader->Filer->USAddress->StateAbbreviationCd```\n",
    "* **Zip**:     ```Return->ReturnHeader->Filer->USAddress->ZIPCd```\n",
    "* **Officer**: ```Return->ReturnHeader->BusinessOfficerGrp->PersonNm```\n",
    "* **Revenue**: ```Return->ReturnData->IRS990->CYTotalRevenueAmt```\n",
    "* **Mission**: ```Return->ReturnData->IRS990->ActivityOrMissionDesc```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import xmltodict\n",
    "\n",
    "# Load the URLs into a list\n",
    "urls = []\n",
    "with open(\"urls.txt\") as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "output = []\n",
    "for url in urls:\n",
    "    # Get the return data\n",
    "    response = requests.get(url.replace(\"\\n\", \"\"))\n",
    "    # Parse it into a dictionary\n",
    "    return_data = xmltodict.parse(response.content)\n",
    "    # Return->ReturnHeader->Filer->BusinessName->BusinessNameLine1Txt\n",
    "    output_dict = {}\n",
    "    output_dict[\"ein\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"EIN\"]\n",
    "    output_dict[\"name\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"BusinessName\"][\"BusinessNameLine1Txt\"]\n",
    "    if \"USAddress\" in return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"].keys():\n",
    "        output_dict[\"address\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"USAddress\"][\"AddressLine1Txt\"]\n",
    "        output_dict[\"city\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"USAddress\"][\"CityNm\"]\n",
    "        output_dict[\"state\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"USAddress\"][\"StateAbbreviationCd\"]\n",
    "        output_dict[\"zip\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"USAddress\"][\"ZIPCd\"]\n",
    "    output_dict[\"officer\"] = return_data[\"Return\"][\"ReturnHeader\"][\"BusinessOfficerGrp\"][\"PersonNm\"]\n",
    "    output_dict[\"revenue\"] = return_data[\"Return\"][\"ReturnData\"][\"IRS990\"][\"CYTotalRevenueAmt\"]\n",
    "    output_dict[\"mission\"] = return_data[\"Return\"][\"ReturnData\"][\"IRS990\"][\"ActivityOrMissionDesc\"]\n",
    "    output.append(output_dict)\n",
    "    # Limit results for now, so we don't have to wait all day.\n",
    "    if len(output) > 100:\n",
    "        break\n",
    "\n",
    "with open(\"data.txt\", \"w\") as f:\n",
    "    dw = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=[\"ein\", \"name\", \"address\", \"city\", \"state\", \"zip\", \"officer\", \"revenue\", \"mission\"])\n",
    "    dw.writeheader()\n",
    "    dw.writerows(output)\n",
    "\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch\n",
    "\n",
    "Elasticsearch is a \"no-sql\" document-oriented data store that uses an inverted index structure for fast queries.  It also supports, like SQL, aggregations, so you can get quantitative results as well as text search results.\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html\n",
    "\n",
    "You can interact with elasticsearch using HTTP requests, or using a wrapper library like the elasticsearch python library, which makes life a little easier:\n",
    "\n",
    "https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch\n",
    "\n",
    "If you don't have the elasticsearch package installed on your machine, you can use this command to install it:\n",
    "\n",
    "```\n",
    "pip install elasticsearch\n",
    "```\n",
    "\n",
    "Pip is a \"package manager\", that consults an online database (https://pypi.python.org/pypi) for information about registered software and what versions are appropriate for which OS/Python version combination:\n",
    "\n",
    "https://pip.pypa.io/en/stable/quickstart/\n",
    "\n",
    "You can install Elasticsearch on your local machine, or for this class, use the server at fcsearchdev04.  We can load the data we downloaded into the index with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# This line connects to the elastic instance, just like a SQL Server connection (we might need to use the fully-qualified name\n",
    "# or IP address to connect).\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "# The next line deletes an index if it exists.  The ignore parameter tells it to ignore the error if the index doesn't exist yet:\n",
    "es.indices.delete(index='gmg', ignore=[400, 404])\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        # This line create a new document with default field types.\n",
    "        es.create(index=\"gmg\", body=d, doc_type=\"document\", id=d[\"ein\"])\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the data\n",
    "\n",
    "Now that we have some data loaded, let's look at querying it. Elasticsearch supports a data query format analagous to SQL, but with a different syntax, referred to as a \"DSL\" (domain-specific language) in JSON format.\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "\n",
    "q = {\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"mission\" : \"mental\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "r = es.search(index=\"gmg\", body=q)\n",
    "\n",
    "for d in r[\"hits\"][\"hits\"]:\n",
    "    print(d[\"_source\"][\"name\"], d[\"_source\"][\"mission\"])\n",
    "\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variants of these queries.  For example, we can use a fuzzy query to get text with nearby words, to account for slight misspellings:\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#fuzziness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "q = {\n",
    "    \"query\": {\n",
    "        \"fuzzy\": {\"name\": \"haite\"}\n",
    "    }\n",
    "}\n",
    "r = es.search(index=\"gmg\", body=q)\n",
    "for d in r[\"hits\"][\"hits\"]:\n",
    "    print(d[\"_source\"][\"name\"])\n",
    "print (\"All done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing the data\n",
    "\n",
    "Python libraries make it easy to spin up web services to share data.  You can run Python from a web server, but it's more convenient and secure to use a package like Flask or CherryPy that are designed for this purpose.  To create a web service using CherryPy, it is simply a matter of wrapping your function in an object, adding an \"exposed\" property to the function name you want to make available, and starting up a CherryPy web server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import cherrypy\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "\n",
    "class Search():\n",
    "    def search(self, s):\n",
    "        q = {\n",
    "            \"query\": {\n",
    "                \"fuzzy\": {\"name\": s}\n",
    "            }\n",
    "        }\n",
    "        r = es.search(index=\"gmg\", body=q)\n",
    "        result = []\n",
    "        for d in r[\"hits\"][\"hits\"]:\n",
    "            result.append(d)\n",
    "\n",
    "        return json.dumps(result)\n",
    "\n",
    "    search.exposed = True\n",
    "\n",
    "cherrypy.quickstart(Search())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enhance our service a little by adding some more items to our search fields using the multi-match search type:\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import cherrypy\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "\n",
    "class Search():\n",
    "    def search(self, s):\n",
    "        q = {\n",
    "            \"query\": {\n",
    "                \"multi_match\" : {\n",
    "                  \"query\":    s, \n",
    "                  \"fields\": [ \"name\", \"mission\", \"city\", \"state\" ] \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        r = es.search(index=\"gmg\", body=q)\n",
    "        result = []\n",
    "        for d in r[\"hits\"][\"hits\"]:\n",
    "            result.append(d)\n",
    "\n",
    "        return json.dumps(result)\n",
    "\n",
    "    search.exposed = True\n",
    "\n",
    "cherrypy.quickstart(Search())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing the data\n",
    "\n",
    "Now that we have a bare-bones process in place, we can look at enhancing it.  One way to do this would be to use Foundation Center's autoclassification API to add subject information to our search index.\n",
    "\n",
    "https://apibeta.foundationcenter.org/docs/v2.0/console.html#/GET%20/text/autoclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Response' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-823aadd08e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msubject_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"facet\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"subject\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0msubject_str\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"description\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\";\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Response' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "\n",
    "s = requests.Session()\n",
    "s.auth = ('api_test', '79fifth')\n",
    "url = \"https://apibeta.foundationcenter.org//v2.0/text/autoclassification?taxonomy=pcs&threshold=90&chunk_text=false&text=\"\n",
    "output = []\n",
    "field_names = []\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    field_names = dr.fieldnames\n",
    "    for d in dr:\n",
    "        # This line create a new document with default field types.\n",
    "        text = d[\"mission\"] + \" \" + d[\"name\"]\n",
    "        response = s.get(url + text.lower())\n",
    "        response_dict = json.loads(response.text)\n",
    "        subject_str = \"\"\n",
    "        for r in response_dict[\"data\"][\"results\"]:\n",
    "            if r[\"facet\"] == \"subject\":\n",
    "                # Let's just grab the first subject\n",
    "                d[\"subject\"] = r[\"description\"]\n",
    "                break\n",
    "        output.append(d)\n",
    "\n",
    "with open(\"data_with_subjects.txt\", \"w\") as f:\n",
    "    field_names.append(\"subject\")\n",
    "    dw = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=field_names)\n",
    "    dw.writeheader()\n",
    "    dw.writerows(output)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, reusing the code we used before, we can load the new data. To aggregate by subject, we need to define a type other than the default for those fields we want to aggregate by.  Fields, not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "es.indices.delete(index='gmg', ignore=[400, 404])\n",
    "with open(\"data_with_subjects.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        es.create(index=\"gmg\", body=d, doc_type=\"document\", id=d[\"ein\"])\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "q = {\n",
    "  \"size\": 0,\n",
    "  \"aggregations\" : {\n",
    "      \"subjects\" : {\n",
    "          \"terms\" : { \"field\" : \"subject.keyword\" }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "r = es.search(index=\"gmg\", body=q)\n",
    "for d in r[\"aggregations\"][\"subjects\"][\"buckets\"]:\n",
    "    print(d[\"key\"], d[\"doc_count\"])\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 15\n",
      "Education 11\n",
      "Human services 7\n",
      "Elementary and secondary education 6\n",
      "Health 5\n",
      "Secondary education 5\n",
      "Arts and culture 2\n",
      "Community and economic development 2\n",
      "Early childhood education 2\n",
      "Economic development 2\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import cherrypy\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "\n",
    "class Search():\n",
    "    def search(self, s):\n",
    "        q = {\n",
    "            \"query\": {\n",
    "                \"fuzzy\": {\"subject\": s}\n",
    "            }\n",
    "        }\n",
    "        r = es.search(index=\"gmg\", body=q)\n",
    "        result = []\n",
    "        for d in r[\"hits\"][\"hits\"]:\n",
    "            result.append(d)\n",
    "\n",
    "        return json.dumps(result)\n",
    "\n",
    "    search.exposed = True\n",
    "\n",
    "cherrypy.quickstart(Search())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch aggregations can be useful for doing SQL-like queries of documents.  There are many flavors of aggregations.  The terms aggregation is probably a good starting point:\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"http://172.16.7.122:9200\", http_auth=(\"[provided in class]\", \"[provided in class]\"))\n",
    "q = {\n",
    "    \"aggregations\" : {\n",
    "        \"subjects\" : {\n",
    "            \"terms\" : { \"field\" : \"subject\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = es.search(index=\"gmg\", body=q)\n",
    "for d in r[\"hits\"][\"hits\"]:\n",
    "    print(d[\"_source\"][\"name\"])\n",
    "print (\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
