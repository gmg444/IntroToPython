{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first set of notebooks, we went over the basic parts of Python that are commonly used for data processing tasks.  In this set, we'll go over some useful libraries and addons, using some concrete examples. \n",
    "\n",
    "https://aws.amazon.com/public-datasets/irs-990/\n",
    "\n",
    "Downloading and reading XML files 990s for one year only, using:\n",
    "\n",
    "https://s3.amazonaws.com/irs-form-990/index_2016.json\n",
    "\n",
    "-- Open file\n",
    "-- Load into dictionary\n",
    "-- Find dictionary keys - we want ein, org name, mission statement, filer city and state\n",
    "-- filer/businessname\n",
    "-- Insert into elasticsearch index\n",
    "-- Codes - BusinessCode\n",
    "-- ActivityOrMissionDesc\n",
    "-- Grant amounts \n",
    "-- Revenue amounts\n",
    "\n",
    "2016 990s\n",
    "Return->ReturnHeader->Filer->BusinessName->BusinessNameLine1Txt\n",
    "Return->ReturnHeader->Filer->USAddress->CityNm\n",
    "Return->ReturnHeader->Filer->USAddress->StateAbbreviationCd\n",
    "Return->ReturnHeader->Filer->USAddress->ZIPCd\n",
    "Return->ReturnData->IRS990->PrincipalOfficerNm\n",
    "Return->ReturnData->IRS990->USAddress\n",
    "Return->ReturnData->IRS990->USAddress->CityNm\n",
    "Return->ReturnData->IRS990->USAddress->StateAbbreviationCd\n",
    "Return->ReturnData->IRS990->CYContributionsGrantsAmt\n",
    "Return->ReturnData->IRS990->CYProgramServiceRevenueAmt\n",
    "Return->ReturnData->IRS990->CYInvestmentIncomeAmt\n",
    "Return->ReturnData->IRS990->CYOtherRevenueAmt\n",
    "Return->ReturnData->IRS990->CYTotalRevenueAmt\n",
    "Return->ReturnData->IRS990->GrossReceiptsAmt\n",
    "Return->ReturnData->IRS990->Desc\n",
    "Return->ReturnData->IRS990->MissionDesc\n",
    "Return->ReturnHeader->BusinessOfficerGrp->PersonNm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(\"https://s3.amazonaws.com/irs-form-990/index_2016.json\")\n",
    "data = json.loads(response.text)\n",
    "urls = []\n",
    "for return_metadata in data[\"Filings2016\"]:\n",
    "    if return_metadata[\"TaxPeriod\"].startswith(\"2015\") and return_metadata[\"FormType\"] == \"990\":\n",
    "        urls.append(return_metadata[\"URL\"] + \"\\n\")\n",
    "\n",
    "with open(\"urls.txt\", \"w\") as f:\n",
    "    f.writelines(urls)\n",
    "\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be written more simply as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import xmltodict\n",
    "\n",
    "urls = []\n",
    "with open(\"urls.txt\") as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "output = []\n",
    "for url in urls:\n",
    "    response = requests.get(url.replace(\"\\n\", \"\"))\n",
    "    return_data = xmltodict.parse(response.content)\n",
    "    # Return->ReturnHeader->Filer->BusinessName->BusinessNameLine1Txt\n",
    "    output_dict = {}\n",
    "    output_dict[\"ein\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"EIN\"]\n",
    "    output_dict[\"name\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"BusinessName\"][\"BusinessNameLine1Txt\"]\n",
    "    output_dict[\"mission\"] = return_data[\"Return\"][\"ReturnData\"][\"IRS990\"][\"ActivityOrMissionDesc\"]\n",
    "    output.append(output_dict)\n",
    "    if len(output) > 10:\n",
    "        break\n",
    "\n",
    "with open(\"data.txt\", \"w\") as f:\n",
    "    dr = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=[\"ein\", \"name\", \"mission\"])\n",
    "    dr.writerows(output)\n",
    "\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add conditions to the comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import xmltodict\n",
    "\n",
    "urls = []\n",
    "with open(\"urls.txt\") as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "output = []\n",
    "for url in urls:\n",
    "    response = requests.get(url.replace(\"\\n\", \"\"))\n",
    "    return_data = xmltodict.parse(response.content)\n",
    "    # Return->ReturnHeader->Filer->BusinessName->BusinessNameLine1Txt\n",
    "    output_dict = {}\n",
    "    output_dict[\"ein\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"EIN\"]\n",
    "    output_dict[\"name\"] = return_data[\"Return\"][\"ReturnHeader\"][\"Filer\"][\"BusinessName\"][\"BusinessNameLine1Txt\"]\n",
    "    output_dict[\"mission\"] = return_data[\"Return\"][\"ReturnData\"][\"IRS990\"][\"ActivityOrMissionDesc\"]\n",
    "    output.append(output_dict)\n",
    "    if len(output) > 10:\n",
    "        break\n",
    "\n",
    "with open(\"data.txt\", \"w\") as f:\n",
    "    dw = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=[\"ein\", \"name\", \"mission\"])\n",
    "    dw.writeheader()\n",
    "    dw.writerows(output)\n",
    "\n",
    "print (\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we defined functions using this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://fcsearchdev04:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "es.indices.delete(index='gmg', ignore=[400, 404])\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        es.create(index=\"gmg\", body=d, doc_type=\"document\", id=d[\"ein\"])\n",
    "\n",
    "print (\"All done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLORIDA BETA PI BETA PHI FRATERNITY\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://fcsearchdev04:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "\n",
    "q = {\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"mission\" : \"mental\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = es.search(index=\"gmg\", body=q)\n",
    "\n",
    "for d in r[\"hits\"][\"hits\"]:\n",
    "    print(d[\"_source\"][\"name\"])\n",
    "\n",
    "print (\"All done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAITI ENDOWMENT FUND\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://fcsearchdev04:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "q = {\n",
    "    \"query\": {\n",
    "        \"fuzzy\": {\"name\": \"haita\"}\n",
    "    }\n",
    "}\n",
    "r = es.search(index=\"gmg\", body=q)\n",
    "for d in r[\"hits\"][\"hits\"]:\n",
    "    print(d[\"_source\"][\"name\"])\n",
    "print (\"All done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A search service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import cherrypy\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(\"http://fcsearchdev04:9200\", http_auth=(\"elastic\", \"ocelot243kiwi\"))\n",
    "\n",
    "class Search():\n",
    "    def search(self, s):\n",
    "        q = {\n",
    "            \"query\": {\n",
    "                \"fuzzy\": {\"name\": s}\n",
    "            }\n",
    "        }\n",
    "        r = es.search(index=\"gmg\", body=q)\n",
    "        result = []\n",
    "        for d in r[\"hits\"][\"hits\"]:\n",
    "            result.append(d)\n",
    "\n",
    "        return json.dumps(result)\n",
    "\n",
    "    search.exposed = True\n",
    "\n",
    "cherrypy.quickstart(Search())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also supports object-oriented programming.  Classes work like modules, but have a little extra functionality to support more object-oriented design patterns.  Here is a simple python class.  Note the keywords **class**, **object** (optional), **self**, and **__init__**.\n",
    "\n",
    "* **class** indicates that the following code is the class definition.\n",
    "* **object** is the parent class for the current object.\n",
    "* **self** is a reference to the current object, to which member variables can be added at run time.\n",
    "* **init** (preceded and followed by double underscores) is the constructor function called when the object is created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"]\n",
    "        break\n",
    "\n",
    "for sent in nltk.sent_tokenize(sample_text):\n",
    "    print(sent)\n",
    "\n",
    "for sent in nltk.sent_tokenize(sample_text):\n",
    "    print (list(nltk.wordpunct_tokenize(sent)))\n",
    "\n",
    "for sent in nltk.sent_tokenize(sample_text):\n",
    "    print(list(nltk.pos_tag(nltk.word_tokenize(sent))))\n",
    "\n",
    "text = list(nltk.word_tokenize(sample_text))\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"]\n",
    "        break\n",
    "\n",
    "text = list(nltk.word_tokenize(sample_text))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"]\n",
    "        break\n",
    "text = list(nltk.word_tokenize(sample_text))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"]\n",
    "        break\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            yield token\n",
    "\n",
    "print (list(normalize(sample_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print (nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(\"John Smith is from the United States of America and works at Microsoft Research Labs\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defined a specific type of exception to check for.  You can also just use the type \"Exception\" to catch all types.  By referencing the Exception object, you can print out diagnostic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 100\n",
    "y = 0\n",
    "try:\n",
    "    z = x/y\n",
    "except Exception as e:\n",
    "    print (\"Something went wrong!\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _traceback_ module can also help by letting you output more detailed information about the error, like a stack trace (list of calling functions that led to the error), and the line number of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "x = 100\n",
    "y = 0\n",
    "try:\n",
    "    z = x/y\n",
    "except Exception as e:\n",
    "    print (\"Something went wrong!\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the _finally_ clause lets you run some code at the end of your try block, whether or not there was an exception, which is occasionally useful if you want to, say, always return a valid value from your function whether or not there was an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 100\n",
    "y = 0\n",
    "try:\n",
    "    z = x/y\n",
    "    result = str(z)\n",
    "except Exception as e:\n",
    "    print (\"Something went wrong!\", e)\n",
    "    result = \"unknown\"\n",
    "finally:\n",
    "    print (\"The result is: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: can you rewrite one of the examples from earlier that accesses a database or web service, and use Exceptions to handle cases where the database or service can't be reached? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
