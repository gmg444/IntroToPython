{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text\n",
    "\n",
    "In this notebook we'll take a quick look at some additional methods of getting information out of text. Python has many tools for this.  First, there are the built in string functions - it's worth taking a minute to look through them:\n",
    "\n",
    "https://docs.python.org/3.6/library/stdtypes.html?highlight=isdigit#text-sequence-type-str\n",
    "\n",
    "https://docs.python.org/3.6/library/string.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Sample text with NUMBERS 123456\"\n",
    "print(s.upper())\n",
    "print(s.lower())\n",
    "print(s.title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = s.split()\n",
    "print(\"Text\", \"Digit\", \"Title\", \"Upper\", \"Lower\")\n",
    "for w in words:\n",
    "    print(w, w.isdigit(), w.istitle(), w.isupper(), w.islower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also built-in collections of characters that can be handy for text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "print(string.digits)\n",
    "print(repr(string.whitespace))  # repr() returns the printable representation of an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Using these functions, can you take the text in the sample file called \"data_without_delimiters.txt\" and pull out the **eins** and the **zip codes**?  It might make sense to start by splitting each line into words using split(), then looking for the word length and type of characters in each, and printing those out.  If there is time, you might also try to pull out the two-letter state abbreviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data_without_delimiters.txt\") as f:\n",
    "    lines = f.read().splitlines()  # Splitlines implicitly gets rid of \\n newline characters.\n",
    "\n",
    "for line in lines:\n",
    "    ########## Your code here #############\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you need to do some more tricky parsing of strings.  For this, **regular expressions** can be handy:\n",
    "\n",
    "https://docs.python.org/3.6/howto/regex.html\n",
    "\n",
    "https://www.debuggex.com/cheatsheet/regex/python\n",
    "     \n",
    "Regular expressions are terse string pattern matching commands that, when applied to a string, return elements that match the patterns. To use them in Python, you first compile the pattern into a pattern object, then apply that object to a string to find matches. The python library includes a search function, which finds the first occurrence; findall searches the whole string.  \n",
    "\n",
    "A simple regular expression is just a string of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('test')\n",
    "m = p.findall('this string contains one test word')\n",
    "print(m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also special characters that define pattern attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('\\d') # \\d matches decimal.\n",
    "s = \"testing this text with the number 123456\"\n",
    "m = p.findall(s)  \n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('\\d+') # + repeats one or more times.\n",
    "s = \"testing this text with the number 123456\"\n",
    "m = p.findall(s)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile(\"t\\w+t\") # t matches the letter t, \\w matches any w alphanumeric character.\n",
    "s = \"testing this text with the number 123456\"\n",
    "m = p.findall(s)  \n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "s = \"510311790 THE SHEPHERD PLACE INC 1362 S GOVERNORS AVENUE DOVER DE 19904 JAMES MOORE 430457\"\n",
    "p = re.compile('\\d{9}') # Number in brackets looks for a certain number of items.\n",
    "m = p.findall(s)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Using regular expressions, can you pull out the 5-digit zip code, two-letter state, and 9-digit ein from this string?  Refer to the regex links above.  Hint - it might be easiest to do each separately instead of trying to put them all in one expression.  Note that:\n",
    "\n",
    "```\n",
    "\\s finds a single whitespace character\n",
    "{n} finds n occurrences, so \\d{5} finds five-digit numbers\n",
    "[A-Z] matches uppercase letters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "s = \"510311790 THE SHEPHERD PLACE INC 1362 S GOVERNORS AVENUE DOVER DE 19904 JAMES MOORE\"\n",
    "############## Your code here ###############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many packages available for various aspects of text processing (entity extraction and classification are the ones we use most internally).  NLTK is a good place to start because it is relatively straightforward to use and is well documented. \n",
    "\n",
    "http://www.nltk.org/\n",
    "http://www.nltk.org/book\n",
    "\n",
    "You can install it with the pip installer:\n",
    "\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "It relies on a lot of ancillary data, which can be downloaded with this command, from within python (class machines already have this installed):\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main uses of NLTK is to parse words and sentences.  Here are some examples - note that the third example actually appends parts of speech (POS) to each word (reference list of POS tags is here: https://cs.nyu.edu/grishman/jet/guide/PennPOS.html). For more details on how best to use these, it is worth going through the NLTK book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"].lower()\n",
    "        break\n",
    "\n",
    "for sent in nltk.sent_tokenize(sample_text):\n",
    "    print(sent)\n",
    "\n",
    "for sent in nltk.sent_tokenize(sample_text):\n",
    "    print (list(nltk.wordpunct_tokenize(sent)))\n",
    "\n",
    "for sent in nltk.sent_tokenize(sample_text):\n",
    "    print(list(nltk.pos_tag(nltk.word_tokenize(sent))))\n",
    "\n",
    "text = list(nltk.word_tokenize(sample_text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the challenges dealing with text is the many different word endings.  NLTK provides a set of \"stemmers\", that is, tools that strip off different word endings, making it easier to identify words in common between documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"].lower()\n",
    "        break\n",
    "\n",
    "text = list(nltk.word_tokenize(sample_text))\n",
    "print(\" \".join(text))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more expensive operation is to lemmatize words, that is, to reduce them to the root word, taking into account its part of speech (http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"]\n",
    "        break\n",
    "text = list(nltk.word_tokenize(sample_text.lower()))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(sample_text)\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the lemmatizer assumes each word is a noun.If you know the part of speech, you can tell it to lemmatize using that information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"wanderings\", pos='n'))\n",
    "print(lemmatizer.lemmatize(\"wanderings\", pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common set of tasks for pre-processing text is to remove stopwords (words likely to be irrelevant, like \"and\" and \"the\").  This can bold a statement down to its essential terms. This code combines these steps into one function that can boil words down into usable basic forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    results = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            results.append(token)\n",
    "    return(results)\n",
    "\n",
    "sample_text = \"\"\n",
    "with open(\"data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        sample_text = d[\"mission\"]\n",
    "        break\n",
    "\n",
    "print (sample_text)\n",
    "print (list(normalize(sample_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** starting with the code below, can you compare the top 50 most common terms from mission statements in New York City versus the rest of New York State?  Does there seem to be any difference?\n",
    "\n",
    "A couple of Python tools might be useful - the default dictionary, which will initialize each dictionary entry to a given type when the entry is created. These are included in the collections package.  Another is the dictionary .get function, which is a fast way of getting a dictionary value. If the requested key doesn't exist, it returns a default value instead of throwing an exception.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    results = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            results.append(token)\n",
    "    return(results)\n",
    "\n",
    "word_dict = defaultdict(int)\n",
    "with open(\"all_data.txt\") as f:\n",
    "    dr = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    for d in dr:\n",
    "        if d[\"state\"].lower() == \"ny\" and d[\"city\"] == \"New York\":\n",
    "            ############ Your code here ##############\n",
    "            pass\n",
    "\n",
    "sorted_list = sorted(word_dict, key=word_dict.get, reverse=True)\n",
    "print (json.dumps(sorted_list[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Corpora\n",
    "\n",
    "NLTK ships with a variety of document corpora, or collections of documents.  We can also load our own.  A simple way to start is to just load a single file of text. This lets us gather quick statistics about the words in the collection of documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are several built in corpora and corpus readers.  The PLaintextCorpusReader lets us load in plain text files; others include text attributes like POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "nyc = PlaintextCorpusReader(\"./\", \"./nyc.txt\")\n",
    "print(list(nyc.words(\"./nyc.txt\"))[0:100])\n",
    "nys = PlaintextCorpusReader(\"./\", \"./nys.txt\")\n",
    "print(list(nys.words(\"./nys.txt\"))[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK Text class lets us do some interesting explorations of the text. The corcordance function, for example, performs a search for the given token and then also provides the surrounding context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc_text = nltk.text.Text(nyc.words(\"./nyc.txt\"))\n",
    "nyc_text.concordance(\"health\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some context surrounding a word, we can discover similar words, e.g. words that that occur frequently in the same context and with a similar distribution: Distributional similarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nyc_text.similar(\"health\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also uses matplotlib and pylab to display graphs and charts that can show dispersions and frequency.  A dispersion plot can give you a quick picture of how words are distributed in a block of text - are they all clumped together or spread evenly, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc_text = nltk.text.Text(nyc.words(\"./nyc.txt\"))\n",
    "nys_text = nltk.text.Text(nyc.words(\"./nys.txt\"))\n",
    "nyc_text.dispersion_plot([\"community\", \"housing\", \"homeless\"])\n",
    "nys_text.dispersion_plot([\"community\", \"housing\", \"homeless\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore much of the built in corpus, use the following methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the corpus object to parse the text into paragraphs, sentences, or words, or just the raw text.\n",
    "\n",
    "https://github.com/alukach/nltk-experiments/blob/master/natural_language_processing_toolkits_cheatsheet.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc.paras(\"./nyc.txt\")[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc.sents(\"./all_text.txt\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(nyc.words(\"./nyc.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc.raw(\"./all_text.txt\")[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analyses\n",
    "\n",
    "A lot of machine learning and search technology depends on analysis of word frequencies, and NLTK provides some basic tools for calculating these.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts  = nltk.FreqDist(nyc.words(\"./nyc.txt\"))\n",
    "vocab   = len(counts.keys())\n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print(\"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv))\n",
    "\n",
    "counts  = nltk.FreqDist(nys.words(\"./nys.txt\"))\n",
    "vocab   = len(counts.keys())\n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print(\"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc_counts  = nltk.FreqDist(nyc.words(\"./nyc.txt\"))\n",
    "nys_counts  = nltk.FreqDist(nyc.words(\"./nys.txt\"))\n",
    "\n",
    "print(nyc_counts.most_common(40)) # The n most common tokens in the corpus\n",
    "print(nys_counts.most_common(40)) # The n most common tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nyc_counts.hapaxes()[0:10])  # A list of all single-occurrence words\n",
    "print(nys_counts.hapaxes()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nyc_counts.freq('art') * 100) # percentage of the corpus for this token\n",
    "print(nys_counts.freq('art') * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyc_counts.plot(25, cumulative=False)\n",
    "nys_counts.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots are not really very interesting - just looking at the common words doesn't really tell us very much.  What is more useful, is to look at the word frequences and see if they can be used predictively to figure out what class of text they are associated with.  This process is the basis of text classification in machine learning, which is what the internal grant autoclassifier is based on.  \n",
    "\n",
    "To get a quick sense of what this is all about we can look for words that are most likely to differentiate NYC from NYS nonprofits.  These are words that are common in on class of text and uncommon in the other.\n",
    "\n",
    "**Question:** using the tools we have reviewed above, can you figure out what 50 words best differentiate NYC from NYC mission statements?  Using this information, can you predict whether or not nonprofit with these mission statements are located in New York City:\n",
    "\n",
    "\"To support the county fire club\"\n",
    "\n",
    "\"To support new student art\"\n",
    "\n",
    "Here is some starting code - hint: you can iterate through the words and get the difference between the frequencies of the two classes, and put each word and the difference in a tuple.  Then you can sort the list by the absolute value of the difference e.g: \n",
    "\n",
    "sorted_list = sorted(d, key=lambda x: abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "nyc = PlaintextCorpusReader(\"./\", \"./nyc.txt\")\n",
    "nys = PlaintextCorpusReader(\"./\", \"./nys.txt\")\n",
    "\n",
    "nyc_counts  = nltk.FreqDist(nyc.words(\"./nyc.txt\"))\n",
    "nys_counts  = nltk.FreqDist(nyc.words(\"./nys.txt\"))\n",
    "\n",
    "words = list(set(nyc.words(\"./nyc.txt\")))\n",
    "########### Your code here ############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is a big field with many sub-fields. If you are interested in pursuing this further, I would really recommend working through the NLTK toolkit book, then signing up for some of the text mining coursera courses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Answers - don't peek!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out eins, zips and state abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    words = line.split(\" \")\n",
    "    output = []\n",
    "    for i in range(len(words)-1):\n",
    "        w = words[i]\n",
    "        if i == 0:\n",
    "            output.append(w)\n",
    "        elif len(w) == 5 and w.isdigit():\n",
    "            output.append(w)\n",
    "        elif len(w) == 2 and w.isupper() and words[i+1].isdigit() and len(words[i+1]) == 5:\n",
    "            output.append(w)\n",
    "    print(json.dumps(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile('\\d{9}\\s') # Finds the 9-digit number\n",
    "m = p.findall(s)\n",
    "print(m)\n",
    "\n",
    "p = re.compile('\\d{5}\\s') # Finds the 5-digit number\n",
    "m = p.findall(s)\n",
    "print(m)\n",
    "\n",
    "p = re.compile('\\s[A-Z]{2}\\s') # Finds the \n",
    "m = p.findall(s)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            text = d[\"mission\"]\n",
    "            word_list = normalize(text)\n",
    "            for w in word_list:\n",
    "                word_dict[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC/NYS text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "for w in words:\n",
    "    nyc_f = nyc_counts.freq(w)\n",
    "    nys_f = nys_counts.freq(w)\n",
    "    delta = nys_f - nyc_f\n",
    "    d.append([w, delta])\n",
    "    \n",
    "sorted_list = sorted(d, key=lambda x: abs(x[1]), reverse=True)\n",
    "for item in sorted_list[0:50]:\n",
    "    which = \"NYS\"\n",
    "    if item[1] < 0:\n",
    "        which = \"NYC\"\n",
    "    print(item[0], which)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
